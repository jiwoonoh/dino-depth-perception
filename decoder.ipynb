{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unrectified image size S_02: 1.392000e+03 5.120000e+02  \n",
    "- Intrinsic matrix K_02: 9.569475e+02 0.000000e+00 6.939767e+02 0.000000e+00 9.522352e+02 2.386081e+02 0.000000e+00 0.000000e+00 1.000000e+00 \n",
    "- Distortion coefficient D_02: -3.750956e-01 2.076838e-01 4.348525e-04 1.603162e-03 -7.469243e-02\n",
    "- Extrinsic rotation R_02: 9.999838e-01 -5.012736e-03 -2.710741e-03 5.002007e-03 9.999797e-01 -3.950381e-03 2.730489e-03 3.936758e-03 9.999885e-01\n",
    "- Extrinsic translation T_02: 5.989688e-02 -1.367835e-03 4.637624e-03\n",
    "- Rectified image dimensions (this is the dimension jiwoo used, after correct for lens distortion it alters the image dimensions) S_rect_02: 1.224000e+03 3.700000e+02\n",
    "- Rectified image rotation (for the stereo cameras to match) R_rect_02: 9.998691e-01 1.512763e-02 -5.741851e-03 -1.512861e-02 9.998855e-01 -1.287536e-04 5.739247e-03 2.156030e-04 9.999835e-01\n",
    "- Rectified image projection matrix P_rect_02: 7.070493e+02 0.000000e+00 6.040814e+02 4.575831e+01 0.000000e+00 7.070493e+02 1.805066e+02 -3.454157e-01 0.000000e+00 0.000000e+00 1.000000e+00 4.981016e-03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class DictDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory containing .pt files.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.file_paths = sorted(\n",
    "            [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.endswith('.pt')]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return the dictionary from the .pt file.\n",
    "        \"\"\"\n",
    "        file_path = self.file_paths[idx]\n",
    "        data = torch.load(file_path)\n",
    "        return data\n",
    "dataset = DictDataset('./data')\n",
    "# print(dataset[0])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "sample = dataloader.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = torch.load(\"./data/folder_0_pair_0.pt\")\n",
    "data_2 = torch.load(\"./data/folder_0_pair_1.pt\")\n",
    "print(data_1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_1['image_t'].keys())\n",
    "print(data_1['image_t']['processed_image'].shape)\n",
    "image = data_1['image_t']['processed_image']\n",
    "image_t1 = data_1['image_t1']['processed_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(image.shape)\n",
    "# Convert the tensor to a numpy array and then to an image\n",
    "image_np = image.numpy().transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image_np)\n",
    "plt.title(\"Original Image\")\n",
    "plt.show()\n",
    "\n",
    "# Denormalize the image\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "image_np_denorm = std * image_np + mean\n",
    "\n",
    "# Display the denormalized image\n",
    "plt.imshow(image_np_denorm)\n",
    "plt.title(\"Denormalized Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pose estimation network to predict 6-DoF poses for source images relative to the target.\n",
    "\n",
    "    Args:\n",
    "        input_channels: Number of input channels (target image + source image stack).\n",
    "    Returns:\n",
    "        pose_final: Predicted 6-DoF poses for source images relative to the target.\n",
    "                    Shape: [batch_size, num_source, 6]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels):\n",
    "        super(PoseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.pose_pred = nn.Conv2d(256, 6, kernel_size=1, stride=1)\n",
    "\n",
    "        self.num_source = None  # Will be set based on input dimensions\n",
    "        \n",
    "\n",
    "    def forward(self, tgt_image, src_image_stack):\n",
    "        # Concatenate target and source images along the channel axis\n",
    "        inputs = torch.cat((tgt_image, src_image_stack), dim=1)\n",
    "        # self.num_source = src_image_stack.shape[1] // (3 * tgt_image.shape[1])\n",
    "\n",
    "        # Forward pass through convolutional layers\n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "\n",
    "        # Predict 6-DoF poses\n",
    "        pose_pred = self.pose_pred(x)  # Shape: [batch_size, 6 * num_source, H, W]\n",
    "\n",
    "        # Average spatial dimensions and reshape\n",
    "        pose_avg = torch.mean(pose_pred, dim=(2, 3))  # Shape: [batch_size, 6 * num_source]\n",
    "        print(pose_avg.shape)\n",
    "        print(pose_avg)\n",
    "        pose_final = 0.01 * pose_avg #.view(-1, self.num_source, 6)  # Shape: [batch_size, num_source, 6]\n",
    "\n",
    "        return pose_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posenet = PoseNet(6)\n",
    "\n",
    "pose_final = posenet(sample['image_t']['processed_image'], sample['image_t1']['processed_image'])\n",
    "\n",
    "print(\"Predicted 6-DoF poses:\")\n",
    "print(pose_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.keys())\n",
    "print(sample['image_t'].keys())\n",
    "inputs = torch.cat((image, image_t1), dim=0)\n",
    "print(inputs.shape)\n",
    "conv1 = nn.Conv2d(6, 16, kernel_size=7, stride=2, padding=3)\n",
    "out = conv1(inputs)\n",
    "\n",
    "\n",
    "# self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2)\n",
    "# self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "# self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "# self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "# self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "# self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "# self.pose_pred = nn.Conv2d(256, 6, kernel_size=1, stride=1)\n",
    "\n",
    "# self.num_source = None  # Will be set based on input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jiwoo\n",
    "import torch.nn as nn\n",
    "class DepthDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=384, output_size=(224, 224)):\n",
    "        super(DepthDecoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.upsample = nn.Upsample(size=output_size, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        depth_map = self.upsample(x)\n",
    "        depth_map.squeeze_(dim=1)\n",
    "        return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DepthDecoder()\n",
    "depth_map = decoder(sample['image_t']['feature_embedding'])\n",
    "print(depth_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_loss\n",
    "image_t = sample['image_t']['processed_image']\n",
    "image_t1 = sample['image_t1']['processed_image']\n",
    "intrinsics_flat = [9.569475e+02, 0.000000e+00, 6.939767e+02,\n",
    "                   0.000000e+00, 9.522352e+02, 2.386081e+02,\n",
    "                   0.000000e+00, 0.000000e+00, 1.000000e+00]\n",
    "\n",
    "# Convert to a 3x3 matrix\n",
    "intrinsics_matrix = torch.tensor(intrinsics_flat).view(1, 3, 3)\n",
    "\n",
    "compute_loss(pred_depth=depth_map, pred_poses=pose_final, tgt_image=image_t1, src_image_stack=image_t,intrinsics=intrinsics_matrix, opt=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gpt generated!\n",
    "class DepthDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder for generating depth maps from encoded features.\n",
    "    Args:\n",
    "        encoder_channels: List of channel sizes from the encoder at each stage (e.g., [64, 128, 256, 512]).\n",
    "    Returns:\n",
    "        depth_map: Single-channel depth map of the input frame's spatial resolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_channels, output_channels=1):\n",
    "        super(DepthDecoder, self).__init__()\n",
    "\n",
    "        # Reverse the encoder channels to use skip connections\n",
    "        self.encoder_channels = encoder_channels[::-1]\n",
    "        \n",
    "        # Create upsampling layers\n",
    "        self.upconv1 = nn.ConvTranspose2d(self.encoder_channels[0], 256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        # Combine skip connections with upsampled features\n",
    "        self.conv1 = nn.Conv2d(self.encoder_channels[1] + 256, 256, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(self.encoder_channels[2] + 128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(self.encoder_channels[3] + 64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final layer to produce depth map\n",
    "        self.final_conv = nn.Conv2d(32, output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: List of encoder features at each resolution (from deepest to shallowest).\n",
    "                      The first element is the deepest layer's features.\n",
    "        Returns:\n",
    "            depth_map: Predicted single-channel depth map.\n",
    "        \"\"\"\n",
    "        x = features[0]  # Start with the deepest feature map\n",
    "\n",
    "        # First upsample\n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat((x, features[1]), dim=1)  # Add skip connection\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        # Second upsample\n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat((x, features[2]), dim=1)  # Add skip connection\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        # Third upsample\n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat((x, features[3]), dim=1)  # Add skip connection\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Final upsample\n",
    "        x = self.upconv4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Final depth map\n",
    "        depth_map = torch.sigmoid(self.final_conv(x))  # Use sigmoid for normalized depth map\n",
    "        return depth_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
