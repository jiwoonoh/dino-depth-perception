{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, dataset_dir, batch_size, img_height, img_width, num_source):\n",
    "        \"\"\"\n",
    "        Initialize the DataLoader object.\n",
    "\n",
    "        Args:\n",
    "            dataset_dir: Root directory of the dataset.\n",
    "            batch_size: Number of samples per batch.\n",
    "            img_height: Height of the images.\n",
    "            img_width: Width of the images.\n",
    "            num_source: Number of source frames to load.\n",
    "        \"\"\"\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.num_source = num_source\n",
    "\n",
    "    def load_train_batch(self, split='train'):\n",
    "        \"\"\"\n",
    "        Load a batch of training instances.\n",
    "\n",
    "        Args:\n",
    "            split: Dataset split to use ('train', 'val', or 'test').\n",
    "\n",
    "        Returns:\n",
    "            tgt_images: Target images tensor of shape [batch_size, 3, H, W].\n",
    "            src_image_stacks: Source image stacks tensor of shape [batch_size, num_source * 3, H, W].\n",
    "            intrinsics: Camera intrinsics tensor of shape [batch_size, 3, 3].\n",
    "        \"\"\"\n",
    "        # Load file paths for images and camera intrinsics\n",
    "        file_list = self.format_file_list(self.dataset_dir, split)\n",
    "\n",
    "        # Shuffle file list for random sampling\n",
    "        combined = list(zip(file_list['image_file_list'], file_list['cam_file_list']))\n",
    "        random.shuffle(combined)\n",
    "        image_file_list, cam_file_list = zip(*combined)\n",
    "\n",
    "        # Initialize storage for batches\n",
    "        tgt_images = []\n",
    "        src_image_stacks = []\n",
    "        intrinsics = []\n",
    "\n",
    "        # Load images and intrinsics for each batch\n",
    "        for i in range(self.batch_size):\n",
    "            tgt_image, src_image_stack = self.unpack_image_sequence(image_file_list[i])\n",
    "            intrinsic = self.load_intrinsics(cam_file_list[i])\n",
    "            tgt_images.append(tgt_image)\n",
    "            src_image_stacks.append(src_image_stack)\n",
    "            intrinsics.append(intrinsic)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        tgt_images = torch.stack(tgt_images, dim=0)\n",
    "        src_image_stacks = torch.stack(src_image_stacks, dim=0)\n",
    "        intrinsics = torch.stack(intrinsics, dim=0)\n",
    "\n",
    "        return tgt_images, src_image_stacks, intrinsics\n",
    "\n",
    "    def format_file_list(self, data_root, split):\n",
    "        \"\"\"\n",
    "        Format file paths into separate lists for images and camera intrinsics.\n",
    "\n",
    "        Args:\n",
    "            data_root: Root directory of the dataset.\n",
    "            split: Dataset split ('train', 'val', 'test').\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing image and camera intrinsic file lists.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(data_root, f'{split}.txt'), 'r') as f:\n",
    "            frames = f.readlines()\n",
    "        subfolders = [x.split(' ')[0] for x in frames]\n",
    "        frame_ids = [x.split(' ')[1].strip() for x in frames]\n",
    "        image_file_list = [os.path.join(data_root, subfolders[i], f\"{frame_ids[i]}.jpg\") for i in range(len(frames))]\n",
    "        cam_file_list = [os.path.join(data_root, subfolders[i], f\"{frame_ids[i]}_cam.txt\") for i in range(len(frames))]\n",
    "        \n",
    "        return {'image_file_list': image_file_list, 'cam_file_list': cam_file_list}\n",
    "\n",
    "    def unpack_image_sequence(self, image_file):\n",
    "        \"\"\"\n",
    "        Unpack an image sequence into target and source images.\n",
    "\n",
    "        Args:\n",
    "            image_file: Path to the image sequence.\n",
    "\n",
    "        Returns:\n",
    "            tgt_image: Target image tensor of shape [3, H, W].\n",
    "            src_image_stack: Source image stack tensor of shape [num_source * 3, H, W].\n",
    "        \"\"\"\n",
    "        # Open the image file\n",
    "        image_seq = Image.open(image_file)\n",
    "        image_seq = np.array(image_seq)  # Convert to NumPy array\n",
    "\n",
    "        # Split image sequence\n",
    "        tgt_start_idx = self.img_width * (self.num_source // 2)\n",
    "        tgt_image = image_seq[:, tgt_start_idx:tgt_start_idx + self.img_width, :]\n",
    "        src_image_1 = image_seq[:, :tgt_start_idx, :]\n",
    "        src_image_2 = image_seq[:, tgt_start_idx + self.img_width:, :]\n",
    "\n",
    "        src_image_seq = np.concatenate([src_image_1, src_image_2], axis=1)\n",
    "        src_image_stack = np.concatenate(\n",
    "            [src_image_seq[:, i * self.img_width:(i + 1) * self.img_width, :] for i in range(self.num_source)], axis=2)\n",
    "\n",
    "        # Transpose dimensions to [C, H, W] for PyTorch\n",
    "        tgt_image = torch.from_numpy(tgt_image).permute(2, 0, 1).float() / 255.0\n",
    "        src_image_stack = torch.from_numpy(src_image_stack).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        return tgt_image, src_image_stack\n",
    "\n",
    "    def load_intrinsics(self, cam_file):\n",
    "        \"\"\"\n",
    "        Load camera intrinsics from a file.\n",
    "\n",
    "        Args:\n",
    "            cam_file: Path to the camera intrinsic file.\n",
    "\n",
    "        Returns:\n",
    "            intrinsics: Camera intrinsics tensor of shape [3, 3].\n",
    "        \"\"\"\n",
    "        with open(cam_file, 'r') as f:\n",
    "            raw_cam_vec = [float(x) for x in f.read().strip().split(',')]\n",
    "        intrinsics = np.array(raw_cam_vec).reshape(3, 3)\n",
    "        \n",
    "        return torch.from_numpy(intrinsics).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PoseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pose estimation network to predict 6-DoF poses for source images relative to the target.\n",
    "\n",
    "    Args:\n",
    "        input_channels: Number of input channels (target image + source image stack).\n",
    "    Returns:\n",
    "        pose_final: Predicted 6-DoF poses for source images relative to the target.\n",
    "                    Shape: [batch_size, num_source, 6]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels):\n",
    "        super(PoseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.pose_pred = nn.Conv2d(256, 6, kernel_size=1, stride=1)\n",
    "\n",
    "        self.num_source = None  # Will be set based on input dimensions\n",
    "\n",
    "    def forward(self, tgt_image, src_image_stack):\n",
    "        # Concatenate target and source images along the channel axis\n",
    "        inputs = torch.cat((tgt_image, src_image_stack), dim=1)\n",
    "        self.num_source = src_image_stack.shape[1] // (3 * tgt_image.shape[1])\n",
    "\n",
    "        # Forward pass through convolutional layers\n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "\n",
    "        # Predict 6-DoF poses\n",
    "        pose_pred = self.pose_pred(x)  # Shape: [batch_size, 6 * num_source, H, W]\n",
    "\n",
    "        # Average spatial dimensions and reshape\n",
    "        pose_avg = torch.mean(pose_pred, dim=(2, 3))  # Shape: [batch_size, 6 * num_source]\n",
    "        pose_final = 0.01 * pose_avg.view(-1, self.num_source, 6)  # Shape: [batch_size, num_source, 6]\n",
    "\n",
    "        return pose_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_to_matrix(vec_rot):\n",
    "    \"\"\"Converts Euler angles to rotation matrix\n",
    "    Args:\n",
    "        vec_rot: Euler angles in the order of rx, ry, rz -- [B, 3] torch.tensor\n",
    "    Returns:\n",
    "        A rotation matrix -- [B, 3, 3] torch.tensor\n",
    "    \"\"\"\n",
    "    batch_size = vec_rot.shape[0]\n",
    "    rx, ry, rz = vec_rot[:, 0], vec_rot[:, 1], vec_rot[:, 2]\n",
    "    \n",
    "    cos_rx, sin_rx = torch.cos(rx), torch.sin(rx)\n",
    "    cos_ry, sin_ry = torch.cos(ry), torch.sin(ry)\n",
    "    cos_rz, sin_rz = torch.cos(rz), torch.sin(rz)\n",
    "    \n",
    "    R_x = torch.stack([torch.ones(batch_size), torch.zeros(batch_size), torch.zeros(batch_size),\n",
    "                       torch.zeros(batch_size), cos_rx, -sin_rx,\n",
    "                       torch.zeros(batch_size), sin_rx, cos_rx], dim=1).view(batch_size, 3, 3)\n",
    "    \n",
    "    R_y = torch.stack([cos_ry, torch.zeros(batch_size), sin_ry,\n",
    "                       torch.zeros(batch_size), torch.ones(batch_size), torch.zeros(batch_size),\n",
    "                       -sin_ry, torch.zeros(batch_size), cos_ry], dim=1).view(batch_size, 3, 3)\n",
    "    \n",
    "    R_z = torch.stack([cos_rz, -sin_rz, torch.zeros(batch_size),\n",
    "                       sin_rz, cos_rz, torch.zeros(batch_size),\n",
    "                       torch.zeros(batch_size), torch.zeros(batch_size), torch.ones(batch_size)], dim=1).view(batch_size, 3, 3)\n",
    "    \n",
    "    rotation_matrix = torch.bmm(R_z, torch.bmm(R_y, R_x))\n",
    "    \n",
    "    return rotation_matrix\n",
    "\n",
    "def dof_vec_to_matrix(dof_vec):\n",
    "    \"\"\"Converts 6DoF parameters to transformation matrix\n",
    "    Args:\n",
    "        vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6] torch.tensor\n",
    "    Returns:\n",
    "        A transformation matrix -- [B, 4, 4] torch.tensor\n",
    "        R11 R12 R13 tx\n",
    "        R21 R22 R23 ty\n",
    "        R31 R32 R33 tz\n",
    "        0   0   0   1\n",
    "    \"\"\"\n",
    "    batch_size = dof_vec.shape[0]\n",
    "    translation = dof_vec[:,:3]\n",
    "    # Add a one at the end of translation\n",
    "    ones = torch.ones(batch_size, 1)\n",
    "    translation = torch.cat((translation, ones), dim=1)\n",
    "    rot_vec = dof_vec[:, 3:]\n",
    "    # print(\"rot_vec\", rot_vec)\n",
    "    rot_matrix = euler_to_matrix(rot_vec)\n",
    "    # add zero at 4 row\n",
    "    zeros = torch.zeros(batch_size, 1, 3)\n",
    "    rot_matrix = torch.cat((rot_matrix, zeros), dim=1)\n",
    "    transformation_matrix = torch.cat((rot_matrix, translation.unsqueeze(2)), dim=2)\n",
    "    return transformation_matrix\n",
    "\n",
    "def inverse_dof(dof_vec):\n",
    "    \"\"\"\n",
    "    Computes the inverse of 6DoF parameters.\n",
    "    \n",
    "    Args:\n",
    "        dof_vec: Tensor of shape [B, 6], representing 6DoF parameters (tx, ty, tz, rx, ry, rz).\n",
    "    \n",
    "    Returns:\n",
    "        Inverted 6DoF parameters: Tensor of shape [B, 6].\n",
    "    \"\"\"\n",
    "    # Negate both the translation and rotation parts\n",
    "    translation_inv = -dof_vec[:, :3]\n",
    "    rotation_inv = -dof_vec[:, 3:]\n",
    "    return torch.cat((translation_inv, rotation_inv), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_cloud(I_t, dof_vec):\n",
    "    \"\"\"\n",
    "    Applies a 6DoF transformation to a point cloud.\n",
    "    \n",
    "    Args:\n",
    "        I_t: Tensor of shape [B, N, 3], representing a batch of point clouds.\n",
    "        dof_vec: Tensor of shape [B, 6], representing 6DoF parameters (tx, ty, tz, rx, ry, rz).\n",
    "    \n",
    "    Returns:\n",
    "        I_t_1: Transformed point cloud, Tensor of shape [B, N, 3].\n",
    "    \"\"\"\n",
    "    batch_size, num_points = I_t.shape[0], I_t.shape[1]\n",
    "    \n",
    "    # Step 1: Convert to homogeneous coordinates\n",
    "    ones = torch.ones(batch_size, num_points, 1, device=I_t.device)  # [B, N, 1]\n",
    "    I_t_augmented = torch.cat((I_t, ones), dim=2)  # [B, N, 4]\n",
    "    \n",
    "    # Step 2: Get the transformation matrix\n",
    "    transf_mat = dof_vec_to_matrix(dof_vec)  # [B, 4, 4]\n",
    "    \n",
    "    # Step 3: Apply the transformation\n",
    "    # Transpose the transformation matrix for compatibility\n",
    "    transf_mat = transf_mat.transpose(1, 2)  # [B, 4, 4]\n",
    "    I_t_1_homo = torch.bmm(I_t_augmented, transf_mat)  # [B, N, 4]\n",
    "    \n",
    "    # Step 4: Convert back to Cartesian coordinates\n",
    "    I_t_1 = I_t_1_homo[:, :, :3]  # Drop the homogeneous coordinate\n",
    "    \n",
    "    return I_t_1\n",
    "\n",
    "def photo_Loss(I, I_pred):\n",
    "    pass\n",
    "    #return photo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.1725, -0.7454,  3.0000],\n",
      "         [-4.3261, -1.4720,  6.0000]],\n",
      "\n",
      "        [[-6.4609, -2.1796,  9.0000],\n",
      "         [-8.5770, -2.8683, 12.0000]]])\n"
     ]
    }
   ],
   "source": [
    "def pixel_to_3d(points, intrins):\n",
    "    \"\"\"\n",
    "    Converts pixel coordinates and depth to 3D coordinates.\n",
    "    \n",
    "    Args:\n",
    "        points: Tensor of shape [B, N, 3], where B is the batch size, N is the number of points,\n",
    "                and each point is represented as (u, v, w) where u and v are pixel coordinates and w is depth.\n",
    "        intrins: List of intrinsic parameters of the camera.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, N, 3] representing the 3D coordinates.\n",
    "    \"\"\"\n",
    "    fx = intrins[0][0]\n",
    "    fy = intrins[1][1]\n",
    "    cx = intrins[0][2]\n",
    "    cy = intrins[1][2]\n",
    "    \n",
    "    u = points[:, :, 0]\n",
    "    v = points[:, :, 1]\n",
    "    w = points[:, :, 2]\n",
    "    \n",
    "    x = ((u - cx) * w) / fx\n",
    "    y = ((v - cy) * w) / fy\n",
    "    z = w\n",
    "    \n",
    "    return torch.stack((x, y, z), dim=2)\n",
    "\n",
    "# Example usage\n",
    "points_batch = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "intrins_example = [\n",
    "    [956.9475, 0.0, 693.9767],\n",
    "    [0.0, 952.2352, 238.6081],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]\n",
    "\n",
    "points_3d = pixel_to_3d(points_batch, intrins_example)\n",
    "print(points_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9999,  2.0000,  3.0000],\n",
      "         [ 3.9999,  5.0000,  6.0000]],\n",
      "\n",
      "        [[ 7.0000,  8.0000,  9.0000],\n",
      "         [ 9.9999, 11.0000, 12.0000]]])\n"
     ]
    }
   ],
   "source": [
    "def _3d_to_pixel(points_3d, intrins):\n",
    "    \"\"\"\n",
    "    Converts 3D coordinates to pixel coordinates and depth.\n",
    "    \n",
    "    Args:\n",
    "        points_3d: Tensor of shape [B, N, 3], where B is the batch size, N is the number of points,\n",
    "                   and each point is represented as (x, y, z) where x, y, and z are 3D coordinates.\n",
    "        intrins: List of intrinsic parameters of the camera.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, N, 3] representing the pixel coordinates and depth.\n",
    "    \"\"\"\n",
    "    fx = intrins[0][0]\n",
    "    fy = intrins[1][1]\n",
    "    cx = intrins[0][2]\n",
    "    cy = intrins[1][2]\n",
    "    \n",
    "    x = points_3d[:, :, 0]\n",
    "    y = points_3d[:, :, 1]\n",
    "    z = points_3d[:, :, 2]\n",
    "    \n",
    "    u = (x * fx / z) + cx\n",
    "    v = (y * fy / z) + cy\n",
    "    w = z\n",
    "    \n",
    "    return torch.stack((u, v, w), dim=2)\n",
    "\n",
    "# Example usage\n",
    "points_3d_example = points_3d\n",
    "intrins_example = [\n",
    "    [956.9475, 0.0, 693.9767],\n",
    "    [0.0, 952.2352, 238.6081],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]\n",
    "\n",
    "pixels = _3d_to_pixel(points_3d_example, intrins_example)\n",
    "print(pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_t_example\n",
      " tensor([[[2, 7, 6],\n",
      "         [4, 6, 5]]])\n",
      "I_t_example.shape torch.Size([1, 2, 3])\n",
      "dof_vec_example\n",
      " tensor([[0, 4, 0, 3, 8, 4]])\n",
      "dof_vec_example.shape torch.Size([1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4927, 13.0113, -1.2582],\n",
       "         [-1.9954, 11.8566, -3.3604]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test dimensions\n",
    "torch.manual_seed(42)\n",
    "batch = 1\n",
    "num_points = 2\n",
    "I_t_example = torch.randint(0, 10, (batch, num_points, 3))\n",
    "dof_vec_example = torch.randint(0, 10, (batch, 6))\n",
    "\n",
    "print('I_t_example\\n', I_t_example)\n",
    "print('I_t_example.shape', I_t_example.shape)\n",
    "print(\"dof_vec_example\\n\", dof_vec_example)\n",
    "print('dof_vec_example.shape', dof_vec_example.shape)\n",
    "\n",
    "step_cloud(I_t_example, dof_vec_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Point Cloud:\n",
      " tensor([[[0.4340, 0.1371, 0.5117],\n",
      "         [0.1585, 0.0758, 0.2247],\n",
      "         [0.0624, 0.1816, 0.9998],\n",
      "         [0.5944, 0.6541, 0.0337],\n",
      "         [0.1716, 0.3336, 0.5782]],\n",
      "\n",
      "        [[0.0600, 0.2846, 0.2007],\n",
      "         [0.5014, 0.3139, 0.4654],\n",
      "         [0.1612, 0.1568, 0.2083],\n",
      "         [0.3289, 0.1054, 0.9192],\n",
      "         [0.4008, 0.9302, 0.6558]]])\n",
      "Transformed Point Cloud:\n",
      " tensor([[[1.4340, 0.1371, 0.5117],\n",
      "         [1.1585, 0.0758, 0.2247],\n",
      "         [1.0624, 0.1816, 0.9998],\n",
      "         [1.5944, 0.6541, 0.0337],\n",
      "         [1.1716, 0.3336, 0.5782]],\n",
      "\n",
      "        [[0.0600, 1.2846, 0.2007],\n",
      "         [0.5014, 1.3139, 0.4654],\n",
      "         [0.1612, 1.1568, 0.2083],\n",
      "         [0.3289, 1.1054, 0.9192],\n",
      "         [0.4008, 1.9302, 0.6558]]])\n",
      "Recovered Point Cloud:\n",
      " tensor([[[0.4340, 0.1371, 0.5117],\n",
      "         [0.1585, 0.0758, 0.2247],\n",
      "         [0.0624, 0.1816, 0.9998],\n",
      "         [0.5944, 0.6541, 0.0337],\n",
      "         [0.1716, 0.3336, 0.5782]],\n",
      "\n",
      "        [[0.0600, 0.2846, 0.2007],\n",
      "         [0.5014, 0.3139, 0.4654],\n",
      "         [0.1612, 0.1568, 0.2083],\n",
      "         [0.3289, 0.1054, 0.9192],\n",
      "         [0.4008, 0.9302, 0.6558]]])\n"
     ]
    }
   ],
   "source": [
    "# unit test, invertible?\n",
    "# Example Input\n",
    "I_t = torch.rand(2, 5, 3)  # Original Point Cloud\n",
    "dof_vec = torch.tensor([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]], dtype=torch.float32)  # Translation only\n",
    "\n",
    "# Step 1: Transform the point cloud\n",
    "transformed_cloud = step_cloud(I_t, dof_vec)\n",
    "\n",
    "# Step 2: Compute the inverse 6DoF\n",
    "inverse_dof_vec = inverse_dof(dof_vec)\n",
    "\n",
    "# Step 3: Apply the inverse transformation\n",
    "recovered_cloud = step_cloud(transformed_cloud, inverse_dof_vec)\n",
    "\n",
    "# Check if input matches recovered cloud\n",
    "print(\"Original Point Cloud:\\n\", I_t)\n",
    "print(\"Transformed Point Cloud:\\n\", transformed_cloud)\n",
    "print(\"Recovered Point Cloud:\\n\", recovered_cloud)\n",
    "\n",
    "assert torch.allclose(I_t, recovered_cloud, atol=1e-6), \"The original and recovered point clouds do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 7, 6, 4, 6, 5],\n",
      "        [0, 4, 0, 3, 8, 4]])\n",
      "tensor([[[ 0.2724, -0.5668,  0.7775,  2.0000],\n",
      "         [-0.9207, -0.3882,  0.0395,  7.0000],\n",
      "         [ 0.2794, -0.7267, -0.6276,  6.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.0951, -0.8405,  0.5334,  0.0000],\n",
      "         [ 0.1101,  0.5414,  0.8335,  4.0000],\n",
      "         [-0.9894, -0.0205,  0.1440,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "\n",
    "example_input = torch.randint(0, 10, (batch, 6))\n",
    "\n",
    "print(example_input)\n",
    "print(dof_vec_to_matrix(example_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projective_inverse_warp(src_image, depth, pose, intrinsics):\n",
    "    \"\"\"\n",
    "    Warps the source image to the target frame using depth and pose.\n",
    "\n",
    "    Args:\n",
    "        src_image: Source image tensor (shape: [B, H, W, 3]).\n",
    "        depth: Depth map for the target view (shape: [B, H, W]).\n",
    "        pose: 6-DoF pose parameters (shape: [B, 6]).\n",
    "        intrinsics: Camera intrinsics matrix (shape: [B, 3, 3]).\n",
    "\n",
    "    Returns:\n",
    "        warped_image: Source image warped to the target frame (shape: [B, H, W, 3]).\n",
    "    \"\"\"\n",
    "    batch_size, img_height, img_width, _ = src_image.shape\n",
    "\n",
    "    # Step 1: Create pixel grid\n",
    "    u, v = torch.meshgrid(torch.arange(0, img_width, device=src_image.device),\n",
    "                          torch.arange(0, img_height, device=src_image.device))\n",
    "    u = u.flatten().float()\n",
    "    v = v.flatten().float()\n",
    "    pixel_coords = torch.stack([u, v, torch.ones_like(u)], dim=1)  # [HW, 3]\n",
    "    pixel_coords = pixel_coords.unsqueeze(0).expand(batch_size, -1, -1)  # [B, HW, 3]\n",
    "\n",
    "    # Step 2: Backproject pixels to 3D space\n",
    "    cam_coords = pixel_to_3d(pixel_coords, intrinsics)  # [B, HW, 3]\n",
    "    cam_coords = cam_coords * depth.view(batch_size, -1, 1)  # Scale by depth\n",
    "\n",
    "    # Step 3: Apply 6-DoF transformation\n",
    "    cam_coords_transformed = step_cloud(cam_coords, pose)  # [B, HW, 3]\n",
    "\n",
    "    # Step 4: Reproject to 2D space\n",
    "    pixel_coords_proj = _3d_to_pixel(cam_coords_transformed, intrinsics)  # [B, HW, 3]\n",
    "    u_proj = pixel_coords_proj[:, :, 0].view(batch_size, img_height, img_width)\n",
    "    v_proj = pixel_coords_proj[:, :, 1].view(batch_size, img_height, img_width)\n",
    "\n",
    "    # Step 5: Sample from source image\n",
    "    grid = torch.stack([u_proj / img_width * 2 - 1, v_proj / img_height * 2 - 1], dim=-1)  # [B, H, W, 2]\n",
    "    warped_image = torch.nn.functional.grid_sample(src_image, grid, align_corners=False)\n",
    "\n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_smoothness_loss(pred_depth, image):\n",
    "    \"\"\"\n",
    "    Computes edge-aware smoothness loss for the predicted depth map.\n",
    "\n",
    "    Args:\n",
    "        pred_depth: Predicted depth map (Tensor [B, H, W]).\n",
    "        image: Corresponding RGB image for edge awareness (Tensor [B, C, H, W]).\n",
    "\n",
    "    Returns:\n",
    "        smoothness_loss: Edge-aware smoothness loss (Tensor).\n",
    "    \"\"\"\n",
    "    # Ensure pred_depth has shape [B, 1, H, W] for consistency\n",
    "    if pred_depth.dim() == 3:\n",
    "        pred_depth = pred_depth.unsqueeze(1)  # [B, 1, H, W]\n",
    "    \n",
    "    # Convert RGB image to grayscale by taking the mean across channels\n",
    "    grayscale = torch.mean(image, dim=1, keepdim=True)  # [B, 1, H, W]\n",
    "\n",
    "    # Compute gradients of depth map\n",
    "    depth_gradient_x = torch.abs(pred_depth[:, :, :, 1:] - pred_depth[:, :, :, :-1])  # [B, 1, H, W-1]\n",
    "    depth_gradient_y = torch.abs(pred_depth[:, :, 1:, :] - pred_depth[:, :, :-1, :])  # [B, 1, H-1, W]\n",
    "\n",
    "    # Compute gradients of image\n",
    "    image_gradient_x = torch.abs(grayscale[:, :, :, 1:] - grayscale[:, :, :, :-1])  # [B, 1, H, W-1]\n",
    "    image_gradient_y = torch.abs(grayscale[:, :, 1:, :] - grayscale[:, :, :-1, :])  # [B, 1, H-1, W]\n",
    "\n",
    "    # Weight depth gradients with image gradients\n",
    "    # Exponential weighting: edges in image lead to less smoothing\n",
    "    weighted_smoothness_x = depth_gradient_x * torch.exp(-image_gradient_x)\n",
    "    weighted_smoothness_y = depth_gradient_y * torch.exp(-image_gradient_y)\n",
    "\n",
    "    # Compute mean loss\n",
    "    smoothness_loss = torch.mean(weighted_smoothness_x) + torch.mean(weighted_smoothness_y)\n",
    "    \n",
    "    return smoothness_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SSIM metric\n",
    "import torch\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # choose device to run on depending on availability\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "def compute_loss(pred_depth, pred_poses, tgt_image, src_image_stack, intrinsics, opt):\n",
    "    \"\"\"\n",
    "    Computes photometric loss, smoothness loss, and total loss.\n",
    "\n",
    "    Args:\n",
    "        pred_depth: List of predicted depth maps for different scales (List of tensors [B, H, W]).\n",
    "        pred_poses: Predicted 6-DoF poses for source frames (Tensor [B, num_source, 6]).\n",
    "        tgt_image: Target image tensor (shape: [B, 3, H, W]).\n",
    "        src_image_stack: Source image stack (shape: [B, 3*num_source, H, W]).\n",
    "        intrinsics: Camera intrinsics matrix (shape: [B, 3, 3]).\n",
    "        opt: Options object containing hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        total_loss: Combined loss (Tensor).\n",
    "        photometric_loss: Photometric loss (Tensor).\n",
    "        smoothness_loss: Smoothness loss (Tensor).\n",
    "    \"\"\"\n",
    "    # Normalize images to [0, 1] if necessary\n",
    "    if tgt_image.max() > 1.0:\n",
    "        tgt_image = tgt_image.float() / 255.0\n",
    "    if src_image_stack.max() > 1.0:\n",
    "        src_image_stack = src_image_stack.float() / 255.0\n",
    "\n",
    "    # Ensure all tensors are on the correct device\n",
    "    tgt_image = tgt_image.to(device)\n",
    "    src_image_stack = src_image_stack.to(device)\n",
    "    intrinsics = intrinsics.to(device)\n",
    "    pred_poses = pred_poses.to(device)\n",
    "    \n",
    "    photometric_loss = 0.0\n",
    "    smoothness_loss = 0.0\n",
    "\n",
    "    for s in range(len(pred_depth)):\n",
    "        curr_depth = pred_depth[s]  # [B, H, W]\n",
    "\n",
    "        # Resize images for the current scale\n",
    "        scale_factor = 1 / (2 ** s)\n",
    "        curr_tgt_image = F.interpolate(\n",
    "            tgt_image, scale_factor=scale_factor, mode='bilinear', align_corners=False)  # [B, 3, H', W']\n",
    "        curr_src_image_stack = F.interpolate(\n",
    "            src_image_stack, scale_factor=scale_factor, mode='bilinear', align_corners=False)  # [B, 3*num_source, H', W']\n",
    "\n",
    "        for i in range(opt.num_source):\n",
    "            # Extract the current source image\n",
    "            src_image = curr_src_image_stack[:, i*3:(i+1)*3, :, :]  # [B, 3, H', W']\n",
    "\n",
    "            # Warp the source image to the target frame\n",
    "            warped_image = projective_inverse_warp(src_image, curr_depth, pred_poses[:, i, :], intrinsics)  # [B, 3, H', W']\n",
    "\n",
    "            # Compute photometric loss (L1 + SSIM)\n",
    "            l1_loss = F.l1_loss(warped_image, curr_tgt_image, reduction='mean')  # Scalar\n",
    "            ssim_loss = 1 - ssim_metric(warped_image, curr_tgt_image)  # [B]\n",
    "            ssim_loss = ssim_loss.mean()  # Scalar\n",
    "            photometric_loss += (0.85 * l1_loss + 0.15 * ssim_loss)  # Scalar\n",
    "\n",
    "        # Compute smoothness loss for the current scale\n",
    "        smoothness_loss += compute_smoothness_loss(curr_depth, curr_tgt_image)  # Scalar\n",
    "\n",
    "    # Combine photometric and smoothness loss\n",
    "    total_loss = photometric_loss + opt.smooth_weight * smoothness_loss  # Scalar\n",
    "\n",
    "    return total_loss, photometric_loss, smoothness_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
