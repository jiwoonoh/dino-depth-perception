{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def euler_to_matrix(vec_rot):\n",
    "    \"\"\"Converts Euler angles to rotation matrix\n",
    "    Args:\n",
    "        vec_rot: Euler angles in the order of rx, ry, rz -- [B, 3] torch.tensor\n",
    "    Returns:\n",
    "        A rotation matrix -- [B, 3, 3] torch.tensor\n",
    "    \"\"\"\n",
    "    batch_size = vec_rot.shape[0]\n",
    "    rx, ry, rz = vec_rot[:, 0], vec_rot[:, 1], vec_rot[:, 2]\n",
    "    \n",
    "    cos_rx, sin_rx = torch.cos(rx), torch.sin(rx)\n",
    "    cos_ry, sin_ry = torch.cos(ry), torch.sin(ry)\n",
    "    cos_rz, sin_rz = torch.cos(rz), torch.sin(rz)\n",
    "    \n",
    "    R_x = torch.stack([torch.ones(batch_size), torch.zeros(batch_size), torch.zeros(batch_size),\n",
    "                       torch.zeros(batch_size), cos_rx, -sin_rx,\n",
    "                       torch.zeros(batch_size), sin_rx, cos_rx], dim=1).view(batch_size, 3, 3)\n",
    "    \n",
    "    R_y = torch.stack([cos_ry, torch.zeros(batch_size), sin_ry,\n",
    "                       torch.zeros(batch_size), torch.ones(batch_size), torch.zeros(batch_size),\n",
    "                       -sin_ry, torch.zeros(batch_size), cos_ry], dim=1).view(batch_size, 3, 3)\n",
    "    \n",
    "    R_z = torch.stack([cos_rz, -sin_rz, torch.zeros(batch_size),\n",
    "                       sin_rz, cos_rz, torch.zeros(batch_size),\n",
    "                       torch.zeros(batch_size), torch.zeros(batch_size), torch.ones(batch_size)], dim=1).view(batch_size, 3, 3)\n",
    "    \n",
    "    rotation_matrix = torch.bmm(R_z, torch.bmm(R_y, R_x))\n",
    "    \n",
    "    return rotation_matrix\n",
    "\n",
    "def dof_vec_to_matrix(dof_vec):\n",
    "    \"\"\"Converts 6DoF parameters to transformation matrix\n",
    "    Args:\n",
    "        vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6] torch.tensor\n",
    "    Returns:\n",
    "        A transformation matrix -- [B, 4, 4] torch.tensor\n",
    "        R11 R12 R13 tx\n",
    "        R21 R22 R23 ty\n",
    "        R31 R32 R33 tz\n",
    "        0   0   0   1\n",
    "    \"\"\"\n",
    "    batch_size = dof_vec.shape[0]\n",
    "    translation = dof_vec[:,:3]\n",
    "    # Add a one at the end of translation\n",
    "    ones = torch.ones(batch_size, 1)\n",
    "    translation = torch.cat((translation, ones), dim=1)\n",
    "    rot_vec = dof_vec[:, 3:]\n",
    "    # print(\"rot_vec\", rot_vec)\n",
    "    rot_matrix = euler_to_matrix(rot_vec)\n",
    "    # add zero at 4 row\n",
    "    zeros = torch.zeros(batch_size, 1, 3)\n",
    "    rot_matrix = torch.cat((rot_matrix, zeros), dim=1)\n",
    "    transformation_matrix = torch.cat((rot_matrix, translation.unsqueeze(2)), dim=2)\n",
    "    return transformation_matrix\n",
    "\n",
    "def inverse_dof(dof_vec):\n",
    "    \"\"\"\n",
    "    Computes the inverse of 6DoF parameters.\n",
    "    \n",
    "    Args:\n",
    "        dof_vec: Tensor of shape [B, 6], representing 6DoF parameters (tx, ty, tz, rx, ry, rz).\n",
    "    \n",
    "    Returns:\n",
    "        Inverted 6DoF parameters: Tensor of shape [B, 6].\n",
    "    \"\"\"\n",
    "    # Negate both the translation and rotation parts\n",
    "    translation_inv = -dof_vec[:, :3]\n",
    "    rotation_inv = -dof_vec[:, 3:]\n",
    "    return torch.cat((translation_inv, rotation_inv), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_cloud(I_t, dof_vec):\n",
    "    \"\"\"\n",
    "    Applies a 6DoF transformation to a point cloud.\n",
    "    \n",
    "    Args:\n",
    "        I_t: Tensor of shape [B, N, 3], representing a batch of point clouds.\n",
    "        dof_vec: Tensor of shape [B, 6], representing 6DoF parameters (tx, ty, tz, rx, ry, rz).\n",
    "    \n",
    "    Returns:\n",
    "        I_t_1: Transformed point cloud, Tensor of shape [B, N, 3].\n",
    "    \"\"\"\n",
    "    batch_size, num_points = I_t.shape[0], I_t.shape[1]\n",
    "    \n",
    "    # Step 1: Convert to homogeneous coordinates\n",
    "    ones = torch.ones(batch_size, num_points, 1, device=I_t.device)  # [B, N, 1]\n",
    "    I_t_augmented = torch.cat((I_t, ones), dim=2)  # [B, N, 4]\n",
    "    \n",
    "    # Step 2: Get the transformation matrix\n",
    "    transf_mat = dof_vec_to_matrix(dof_vec)  # [B, 4, 4]\n",
    "    \n",
    "    # Step 3: Apply the transformation\n",
    "    # Transpose the transformation matrix for compatibility\n",
    "    transf_mat = transf_mat.transpose(1, 2)  # [B, 4, 4]\n",
    "    I_t_1_homo = torch.bmm(I_t_augmented, transf_mat)  # [B, N, 4]\n",
    "    \n",
    "    # Step 4: Convert back to Cartesian coordinates\n",
    "    I_t_1 = I_t_1_homo[:, :, :3]  # Drop the homogeneous coordinate\n",
    "    \n",
    "    return I_t_1\n",
    "\n",
    "def photo_Loss(I, I_pred):\n",
    "    pass\n",
    "    #return photo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.1725, -0.7454,  3.0000],\n",
      "         [-4.3261, -1.4720,  6.0000]],\n",
      "\n",
      "        [[-6.4609, -2.1796,  9.0000],\n",
      "         [-8.5770, -2.8683, 12.0000]]])\n"
     ]
    }
   ],
   "source": [
    "def pixel_to_3d(points, intrins):\n",
    "    \"\"\"\n",
    "    Converts pixel coordinates and depth to 3D coordinates.\n",
    "    \n",
    "    Args:\n",
    "        points: Tensor of shape [B, N, 3], where B is the batch size, N is the number of points,\n",
    "                and each point is represented as (u, v, w) where u and v are pixel coordinates and w is depth.\n",
    "        intrins: List of intrinsic parameters of the camera.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, N, 3] representing the 3D coordinates.\n",
    "    \"\"\"\n",
    "    fx = intrins[0][0]\n",
    "    fy = intrins[1][1]\n",
    "    cx = intrins[0][2]\n",
    "    cy = intrins[1][2]\n",
    "    \n",
    "    u = points[:, :, 0]\n",
    "    v = points[:, :, 1]\n",
    "    w = points[:, :, 2]\n",
    "    \n",
    "    x = ((u - cx) * w) / fx\n",
    "    y = ((v - cy) * w) / fy\n",
    "    z = w\n",
    "    \n",
    "    return torch.stack((x, y, z), dim=2)\n",
    "\n",
    "# Example usage\n",
    "points_batch = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "intrins_example = [\n",
    "    [956.9475, 0.0, 693.9767],\n",
    "    [0.0, 952.2352, 238.6081],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]\n",
    "\n",
    "points_3d = pixel_to_3d(points_batch, intrins_example)\n",
    "print(points_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9999,  2.0000,  3.0000],\n",
      "         [ 3.9999,  5.0000,  6.0000]],\n",
      "\n",
      "        [[ 7.0000,  8.0000,  9.0000],\n",
      "         [ 9.9999, 11.0000, 12.0000]]])\n"
     ]
    }
   ],
   "source": [
    "def _3d_to_pixel(points_3d, intrins):\n",
    "    \"\"\"\n",
    "    Converts 3D coordinates to pixel coordinates and depth.\n",
    "    \n",
    "    Args:\n",
    "        points_3d: Tensor of shape [B, N, 3], where B is the batch size, N is the number of points,\n",
    "                   and each point is represented as (x, y, z) where x, y, and z are 3D coordinates.\n",
    "        intrins: List of intrinsic parameters of the camera.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, N, 3] representing the pixel coordinates and depth.\n",
    "    \"\"\"\n",
    "    fx = intrins[0][0]\n",
    "    fy = intrins[1][1]\n",
    "    cx = intrins[0][2]\n",
    "    cy = intrins[1][2]\n",
    "    \n",
    "    x = points_3d[:, :, 0]\n",
    "    y = points_3d[:, :, 1]\n",
    "    z = points_3d[:, :, 2]\n",
    "    \n",
    "    u = (x * fx / z) + cx\n",
    "    v = (y * fy / z) + cy\n",
    "    w = z\n",
    "    \n",
    "    return torch.stack((u, v, w), dim=2)\n",
    "\n",
    "# Example usage\n",
    "points_3d_example = points_3d\n",
    "intrins_example = [\n",
    "    [956.9475, 0.0, 693.9767],\n",
    "    [0.0, 952.2352, 238.6081],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]\n",
    "\n",
    "pixels = _3d_to_pixel(points_3d_example, intrins_example)\n",
    "print(pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_t_example\n",
      " tensor([[[2, 7, 6],\n",
      "         [4, 6, 5]]])\n",
      "I_t_example.shape torch.Size([1, 2, 3])\n",
      "dof_vec_example\n",
      " tensor([[0, 4, 0, 3, 8, 4]])\n",
      "dof_vec_example.shape torch.Size([1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4927, 13.0113, -1.2582],\n",
       "         [-1.9954, 11.8566, -3.3604]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test dimensions\n",
    "torch.manual_seed(42)\n",
    "batch = 1\n",
    "num_points = 2\n",
    "I_t_example = torch.randint(0, 10, (batch, num_points, 3))\n",
    "dof_vec_example = torch.randint(0, 10, (batch, 6))\n",
    "\n",
    "print('I_t_example\\n', I_t_example)\n",
    "print('I_t_example.shape', I_t_example.shape)\n",
    "print(\"dof_vec_example\\n\", dof_vec_example)\n",
    "print('dof_vec_example.shape', dof_vec_example.shape)\n",
    "\n",
    "step_cloud(I_t_example, dof_vec_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Point Cloud:\n",
      " tensor([[[0.4340, 0.1371, 0.5117],\n",
      "         [0.1585, 0.0758, 0.2247],\n",
      "         [0.0624, 0.1816, 0.9998],\n",
      "         [0.5944, 0.6541, 0.0337],\n",
      "         [0.1716, 0.3336, 0.5782]],\n",
      "\n",
      "        [[0.0600, 0.2846, 0.2007],\n",
      "         [0.5014, 0.3139, 0.4654],\n",
      "         [0.1612, 0.1568, 0.2083],\n",
      "         [0.3289, 0.1054, 0.9192],\n",
      "         [0.4008, 0.9302, 0.6558]]])\n",
      "Transformed Point Cloud:\n",
      " tensor([[[1.4340, 0.1371, 0.5117],\n",
      "         [1.1585, 0.0758, 0.2247],\n",
      "         [1.0624, 0.1816, 0.9998],\n",
      "         [1.5944, 0.6541, 0.0337],\n",
      "         [1.1716, 0.3336, 0.5782]],\n",
      "\n",
      "        [[0.0600, 1.2846, 0.2007],\n",
      "         [0.5014, 1.3139, 0.4654],\n",
      "         [0.1612, 1.1568, 0.2083],\n",
      "         [0.3289, 1.1054, 0.9192],\n",
      "         [0.4008, 1.9302, 0.6558]]])\n",
      "Recovered Point Cloud:\n",
      " tensor([[[0.4340, 0.1371, 0.5117],\n",
      "         [0.1585, 0.0758, 0.2247],\n",
      "         [0.0624, 0.1816, 0.9998],\n",
      "         [0.5944, 0.6541, 0.0337],\n",
      "         [0.1716, 0.3336, 0.5782]],\n",
      "\n",
      "        [[0.0600, 0.2846, 0.2007],\n",
      "         [0.5014, 0.3139, 0.4654],\n",
      "         [0.1612, 0.1568, 0.2083],\n",
      "         [0.3289, 0.1054, 0.9192],\n",
      "         [0.4008, 0.9302, 0.6558]]])\n"
     ]
    }
   ],
   "source": [
    "# unit test, invertible?\n",
    "# Example Input\n",
    "I_t = torch.rand(2, 5, 3)  # Original Point Cloud\n",
    "dof_vec = torch.tensor([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]], dtype=torch.float32)  # Translation only\n",
    "\n",
    "# Step 1: Transform the point cloud\n",
    "transformed_cloud = step_cloud(I_t, dof_vec)\n",
    "\n",
    "# Step 2: Compute the inverse 6DoF\n",
    "inverse_dof_vec = inverse_dof(dof_vec)\n",
    "\n",
    "# Step 3: Apply the inverse transformation\n",
    "recovered_cloud = step_cloud(transformed_cloud, inverse_dof_vec)\n",
    "\n",
    "# Check if input matches recovered cloud\n",
    "print(\"Original Point Cloud:\\n\", I_t)\n",
    "print(\"Transformed Point Cloud:\\n\", transformed_cloud)\n",
    "print(\"Recovered Point Cloud:\\n\", recovered_cloud)\n",
    "\n",
    "assert torch.allclose(I_t, recovered_cloud, atol=1e-6), \"The original and recovered point clouds do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 7, 6, 4, 6, 5],\n",
      "        [0, 4, 0, 3, 8, 4]])\n",
      "tensor([[[ 0.2724, -0.5668,  0.7775,  2.0000],\n",
      "         [-0.9207, -0.3882,  0.0395,  7.0000],\n",
      "         [ 0.2794, -0.7267, -0.6276,  6.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.0951, -0.8405,  0.5334,  0.0000],\n",
      "         [ 0.1101,  0.5414,  0.8335,  4.0000],\n",
      "         [-0.9894, -0.0205,  0.1440,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "\n",
    "example_input = torch.randint(0, 10, (batch, 6))\n",
    "\n",
    "print(example_input)\n",
    "print(dof_vec_to_matrix(example_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
