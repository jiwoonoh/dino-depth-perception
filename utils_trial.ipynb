{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from __future__ import division\n",
    "import tf_slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference code to load the data - Need to fix the paths to the data\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, \n",
    "                 dataset_dir=None, \n",
    "                 batch_size=None, \n",
    "                 img_height=None, \n",
    "                 img_width=None, \n",
    "                 num_source=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.num_source = num_source\n",
    "\n",
    "    def load_train_batch(self):\n",
    "        \"\"\"Load a batch of training instances.\"\"\"\n",
    "\n",
    "        # Random seed generated to randomize data loading for each epoch\n",
    "        seed = random.randint(0, 2**31 - 1)\n",
    "\n",
    "        # Reads train.txt file from dataset directory\n",
    "        # Lists the paths of images and corresponding camera intrinsic files\n",
    "        # Output = image_file_list, cam_file_list\n",
    "        file_list = self.format_file_list(self.dataset_dir, 'train')\n",
    "\n",
    "        # TensorFlow queues created for image and intrinsic files\n",
    "        # string_input_producer manages input queues that provide file paths one at a time for processing\n",
    "        image_paths_queue = tf.train.string_input_producer(\n",
    "            file_list['image_file_list'], \n",
    "            seed=seed, \n",
    "            shuffle=True)\n",
    "        cam_paths_queue = tf.train.string_input_producer(\n",
    "            file_list['cam_file_list'], \n",
    "            seed=seed, \n",
    "            shuffle=True)\n",
    "        self.steps_per_epoch = int(\n",
    "            len(file_list['image_file_list']) // self.batch_size)\n",
    "\n",
    "        # WholeFileReader reads raw file contents from image_paths_queue\n",
    "        img_reader = tf.WholeFileReader()\n",
    "        _, image_contents = img_reader.read(image_paths_queue)\n",
    "        # Decode JPEG-encoded image sequences into tensors\n",
    "        image_seq = tf.image.decode_jpeg(image_contents)\n",
    "\n",
    "        # Split sequences into target and source images\n",
    "        #   Target image - center frame in sequence\n",
    "        #   Source image stack - frames before and after the target frame, concat along channel axis\n",
    "        tgt_image, src_image_stack = self.unpack_image_sequence(\n",
    "            image_seq, self.img_height, self.img_width, self.num_source)\n",
    "\n",
    "        # Load camera intrinsics\n",
    "        # TextLineReader reads corresponding camera intrinsic files - reshaped into 3x3 intrinsic matrix\n",
    "        cam_reader = tf.TextLineReader()\n",
    "        _, raw_cam_contents = cam_reader.read(cam_paths_queue)\n",
    "        rec_def = [[1.] for _ in range(9)]\n",
    "        raw_cam_vec = tf.decode_csv(raw_cam_contents, record_defaults=rec_def)\n",
    "        raw_cam_vec = tf.stack(raw_cam_vec)\n",
    "        intrinsics = tf.reshape(raw_cam_vec, [3, 3])\n",
    "\n",
    "        # Form training batches - target images, source stacks, intrinsics\n",
    "        src_image_stack, tgt_image, intrinsics = tf.train.batch(\n",
    "            [src_image_stack, tgt_image, intrinsics], \n",
    "            batch_size=self.batch_size)\n",
    "\n",
    "        return tgt_image, src_image_stack, intrinsics\n",
    "\n",
    "    # Construct 3x3 camera intrinsics matrix for each batch\n",
    "    def make_intrinsics_matrix(self, fx, fy, cx, cy):\n",
    "        # Uses focal and principal points\n",
    "        batch_size = fx.get_shape().as_list()[0]\n",
    "        zeros = tf.zeros_like(fx)\n",
    "        r1 = tf.stack([fx, zeros, cx], axis=1)\n",
    "        r2 = tf.stack([zeros, fy, cy], axis=1)\n",
    "        r3 = tf.constant([0., 0., 1.], shape=[1, 3])\n",
    "        r3 = tf.tile(r3, [batch_size, 1])\n",
    "        intrinsics = tf.stack([r1, r2, r3], axis=1)\n",
    "        return intrinsics\n",
    "\n",
    "    # Reads list of file paths and formats them into seaparate lists for image files and camera intrinsic files\n",
    "    def format_file_list(self, data_root, split):\n",
    "        with open(os.path.join(data_root, f'{split}.txt'), 'r') as f:\n",
    "            frames = f.readlines()\n",
    "        subfolders = [x.split(' ')[0] for x in frames]\n",
    "        frame_ids = [x.split(' ')[1].strip() for x in frames]\n",
    "        image_file_list = [os.path.join(data_root, subfolders[i], \n",
    "                                        f\"{frame_ids[i]}.jpg\") for i in range(len(frames))]\n",
    "        cam_file_list = [os.path.join(data_root, subfolders[i], \n",
    "                                      f\"{frame_ids[i]}_cam.txt\") for i in range(len(frames))]\n",
    "        return {'image_file_list': image_file_list, 'cam_file_list': cam_file_list}\n",
    "\n",
    "    # Split single input into target image and source image stack\n",
    "    def unpack_image_sequence(self, image_seq, img_height, img_width, num_source):\n",
    "        # Assuming the center image is the target frame\n",
    "        tgt_start_idx = int(img_width * (num_source // 2))\n",
    "        tgt_image = tf.slice(image_seq, \n",
    "                             [0, tgt_start_idx, 0], \n",
    "                             [-1, img_width, -1])\n",
    "\n",
    "        # Source frames before the target frame\n",
    "        src_image_1 = tf.slice(image_seq, \n",
    "                               [0, 0, 0], \n",
    "                               [-1, int(img_width * (num_source // 2)), -1])\n",
    "\n",
    "        # Source frames after the target frame\n",
    "        src_image_2 = tf.slice(image_seq, \n",
    "                               [0, int(tgt_start_idx + img_width), 0], \n",
    "                               [-1, int(img_width * (num_source // 2)), -1])\n",
    "\n",
    "        src_image_seq = tf.concat([src_image_1, src_image_2], axis=1)\n",
    "\n",
    "        # Stack source frames along the color channels (i.e., [H, W, N*3])\n",
    "        src_image_stack = tf.concat([tf.slice(src_image_seq, \n",
    "                                    [0, i * img_width, 0], \n",
    "                                    [-1, img_width, -1]) \n",
    "                                    for i in range(num_source)], axis=2)\n",
    "        src_image_stack.set_shape([img_height, img_width, num_source * 3])\n",
    "        tgt_image.set_shape([img_height, img_width, 3])\n",
    "\n",
    "        return tgt_image, src_image_stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_exp_net(tgt_image, src_image_stack, is_training=True):\n",
    "    \"\"\"\n",
    "    Pose estimation network to predict 6-DoF poses for source images relative to the target.\n",
    "    Args:\n",
    "        tgt_image: Target image (RGB).\n",
    "        src_image_stack: Stack of source images.\n",
    "        is_training: Training mode.\n",
    "    Returns:\n",
    "        pose_final: Predicted 6-DoF poses for source images relative to the target.\n",
    "                   Shape: [batch_size, num_source, 6]\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate target image and source images along the channel axis\n",
    "    inputs = tf.concat([tgt_image, src_image_stack], axis=3)\n",
    "    num_source = int(src_image_stack.get_shape()[3].value // 3)\n",
    "\n",
    "    # Define namespace of the network for debugging\n",
    "    with tf.variable_scope('pose_net'):\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                            normalizer_fn=None,\n",
    "                            weights_regularizer=slim.l2_regularizer(0.05),\n",
    "                            activation_fn=tf.nn.relu):\n",
    "            # Shared convolutional layers for feature extraction\n",
    "            cnv1 = slim.conv2d(inputs, 16, [7, 7], stride=2, scope='cnv1')\n",
    "            cnv2 = slim.conv2d(cnv1, 32, [5, 5], stride=2, scope='cnv2')\n",
    "            cnv3 = slim.conv2d(cnv2, 64, [3, 3], stride=2, scope='cnv3')\n",
    "            cnv4 = slim.conv2d(cnv3, 128, [3, 3], stride=2, scope='cnv4')\n",
    "            cnv5 = slim.conv2d(cnv4, 256, [3, 3], stride=2, scope='cnv5')\n",
    "            cnv6 = slim.conv2d(cnv5, 256, [3, 3], stride=2, scope='cnv6')\n",
    "            cnv7 = slim.conv2d(cnv6, 256, [3, 3], stride=2, scope='cnv7')\n",
    "\n",
    "            # Predict 6-DoF poses (translation + rotation)\n",
    "            pose_pred = slim.conv2d(cnv7, 6 * num_source, [1, 1], scope='pred',\n",
    "                                    stride=1, normalizer_fn=None, activation_fn=None)\n",
    "\n",
    "            # Average spatial dimensions and scale pose predictions\n",
    "            pose_avg = tf.reduce_mean(pose_pred, [1, 2])  # Average spatial dimensions\n",
    "            pose_final = 0.01 * tf.reshape(pose_avg, [-1, num_source, 6])  # Final 6-DoF poses\n",
    "\n",
    "            return pose_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def euler_to_matrix(vec_rot):\n",
    "    \"\"\"Converts Euler angles to rotation matrix\n",
    "    Args:\n",
    "        vec_rot: Euler angles in the order of rx, ry, rz -- [B, 3] torch.tensor\n",
    "    Returns:\n",
    "        A rotation matrix -- [B, 3, 3] torch.tensor\n",
    "    \"\"\"\n",
    "    batch_size = vec_rot.shape[0]\n",
    "    rx, ry, rz = vec_rot[:, 0], vec_rot[:, 1], vec_rot[:, 2]\n",
    "    \n",
    "    cos_rx, sin_rx = torch.cos(rx), torch.sin(rx)\n",
    "    cos_ry, sin_ry = torch.cos(ry), torch.sin(ry)\n",
    "    cos_rz, sin_rz = torch.cos(rz), torch.sin(rz)\n",
    "    \n",
    "    R_x = torch.stack([torch.ones(batch_size), torch.zeros(batch_size), torch.zeros(batch_size),\n",
    "                       torch.zeros(batch_size), cos_rx, -sin_rx,\n",
    "                       torch.zeros(batch_size), sin_rx, cos_rx], dim=1).view(batch_size, 3, 3)\n",
    "    \n",
    "    R_y = torch.stack([cos_ry, torch.zeros(batch_size), sin_ry,\n",
    "                       torch.zeros(batch_size), torch.ones(batch_size), torch.zeros(batch_size),\n",
    "                       -sin_ry, torch.zeros(batch_size), cos_ry], dim=1).view(batch_size, 3, 3)\n",
    "    \n",
    "    R_z = torch.stack([cos_rz, -sin_rz, torch.zeros(batch_size),\n",
    "                       sin_rz, cos_rz, torch.zeros(batch_size),\n",
    "                       torch.zeros(batch_size), torch.zeros(batch_size), torch.ones(batch_size)], dim=1).view(batch_size, 3, 3)\n",
    "    \n",
    "    rotation_matrix = torch.bmm(R_z, torch.bmm(R_y, R_x))\n",
    "    \n",
    "    return rotation_matrix\n",
    "\n",
    "def dof_vec_to_matrix(dof_vec):\n",
    "    \"\"\"Converts 6DoF parameters to transformation matrix\n",
    "    Args:\n",
    "        vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6] torch.tensor\n",
    "    Returns:\n",
    "        A transformation matrix -- [B, 4, 4] torch.tensor\n",
    "        R11 R12 R13 tx\n",
    "        R21 R22 R23 ty\n",
    "        R31 R32 R33 tz\n",
    "        0   0   0   1\n",
    "    \"\"\"\n",
    "    batch_size = dof_vec.shape[0]\n",
    "    translation = dof_vec[:,:3]\n",
    "    # Add a one at the end of translation\n",
    "    ones = torch.ones(batch_size, 1)\n",
    "    translation = torch.cat((translation, ones), dim=1)\n",
    "    rot_vec = dof_vec[:, 3:]\n",
    "    # print(\"rot_vec\", rot_vec)\n",
    "    rot_matrix = euler_to_matrix(rot_vec)\n",
    "    # add zero at 4 row\n",
    "    zeros = torch.zeros(batch_size, 1, 3)\n",
    "    rot_matrix = torch.cat((rot_matrix, zeros), dim=1)\n",
    "    transformation_matrix = torch.cat((rot_matrix, translation.unsqueeze(2)), dim=2)\n",
    "    return transformation_matrix\n",
    "\n",
    "def inverse_dof(dof_vec):\n",
    "    \"\"\"\n",
    "    Computes the inverse of 6DoF parameters.\n",
    "    \n",
    "    Args:\n",
    "        dof_vec: Tensor of shape [B, 6], representing 6DoF parameters (tx, ty, tz, rx, ry, rz).\n",
    "    \n",
    "    Returns:\n",
    "        Inverted 6DoF parameters: Tensor of shape [B, 6].\n",
    "    \"\"\"\n",
    "    # Negate both the translation and rotation parts\n",
    "    translation_inv = -dof_vec[:, :3]\n",
    "    rotation_inv = -dof_vec[:, 3:]\n",
    "    return torch.cat((translation_inv, rotation_inv), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_cloud(I_t, dof_vec):\n",
    "    \"\"\"\n",
    "    Applies a 6DoF transformation to a point cloud.\n",
    "    \n",
    "    Args:\n",
    "        I_t: Tensor of shape [B, N, 3], representing a batch of point clouds.\n",
    "        dof_vec: Tensor of shape [B, 6], representing 6DoF parameters (tx, ty, tz, rx, ry, rz).\n",
    "    \n",
    "    Returns:\n",
    "        I_t_1: Transformed point cloud, Tensor of shape [B, N, 3].\n",
    "    \"\"\"\n",
    "    batch_size, num_points = I_t.shape[0], I_t.shape[1]\n",
    "    \n",
    "    # Step 1: Convert to homogeneous coordinates\n",
    "    ones = torch.ones(batch_size, num_points, 1, device=I_t.device)  # [B, N, 1]\n",
    "    I_t_augmented = torch.cat((I_t, ones), dim=2)  # [B, N, 4]\n",
    "    \n",
    "    # Step 2: Get the transformation matrix\n",
    "    transf_mat = dof_vec_to_matrix(dof_vec)  # [B, 4, 4]\n",
    "    \n",
    "    # Step 3: Apply the transformation\n",
    "    # Transpose the transformation matrix for compatibility\n",
    "    transf_mat = transf_mat.transpose(1, 2)  # [B, 4, 4]\n",
    "    I_t_1_homo = torch.bmm(I_t_augmented, transf_mat)  # [B, N, 4]\n",
    "    \n",
    "    # Step 4: Convert back to Cartesian coordinates\n",
    "    I_t_1 = I_t_1_homo[:, :, :3]  # Drop the homogeneous coordinate\n",
    "    \n",
    "    return I_t_1\n",
    "\n",
    "def photo_Loss(I, I_pred):\n",
    "    pass\n",
    "    #return photo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.1725, -0.7454,  3.0000],\n",
      "         [-4.3261, -1.4720,  6.0000]],\n",
      "\n",
      "        [[-6.4609, -2.1796,  9.0000],\n",
      "         [-8.5770, -2.8683, 12.0000]]])\n"
     ]
    }
   ],
   "source": [
    "def pixel_to_3d(points, intrins):\n",
    "    \"\"\"\n",
    "    Converts pixel coordinates and depth to 3D coordinates.\n",
    "    \n",
    "    Args:\n",
    "        points: Tensor of shape [B, N, 3], where B is the batch size, N is the number of points,\n",
    "                and each point is represented as (u, v, w) where u and v are pixel coordinates and w is depth.\n",
    "        intrins: List of intrinsic parameters of the camera.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, N, 3] representing the 3D coordinates.\n",
    "    \"\"\"\n",
    "    fx = intrins[0][0]\n",
    "    fy = intrins[1][1]\n",
    "    cx = intrins[0][2]\n",
    "    cy = intrins[1][2]\n",
    "    \n",
    "    u = points[:, :, 0]\n",
    "    v = points[:, :, 1]\n",
    "    w = points[:, :, 2]\n",
    "    \n",
    "    x = ((u - cx) * w) / fx\n",
    "    y = ((v - cy) * w) / fy\n",
    "    z = w\n",
    "    \n",
    "    return torch.stack((x, y, z), dim=2)\n",
    "\n",
    "# Example usage\n",
    "points_batch = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "intrins_example = [\n",
    "    [956.9475, 0.0, 693.9767],\n",
    "    [0.0, 952.2352, 238.6081],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]\n",
    "\n",
    "points_3d = pixel_to_3d(points_batch, intrins_example)\n",
    "print(points_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9999,  2.0000,  3.0000],\n",
      "         [ 3.9999,  5.0000,  6.0000]],\n",
      "\n",
      "        [[ 7.0000,  8.0000,  9.0000],\n",
      "         [ 9.9999, 11.0000, 12.0000]]])\n"
     ]
    }
   ],
   "source": [
    "def _3d_to_pixel(points_3d, intrins):\n",
    "    \"\"\"\n",
    "    Converts 3D coordinates to pixel coordinates and depth.\n",
    "    \n",
    "    Args:\n",
    "        points_3d: Tensor of shape [B, N, 3], where B is the batch size, N is the number of points,\n",
    "                   and each point is represented as (x, y, z) where x, y, and z are 3D coordinates.\n",
    "        intrins: List of intrinsic parameters of the camera.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, N, 3] representing the pixel coordinates and depth.\n",
    "    \"\"\"\n",
    "    fx = intrins[0][0]\n",
    "    fy = intrins[1][1]\n",
    "    cx = intrins[0][2]\n",
    "    cy = intrins[1][2]\n",
    "    \n",
    "    x = points_3d[:, :, 0]\n",
    "    y = points_3d[:, :, 1]\n",
    "    z = points_3d[:, :, 2]\n",
    "    \n",
    "    u = (x * fx / z) + cx\n",
    "    v = (y * fy / z) + cy\n",
    "    w = z\n",
    "    \n",
    "    return torch.stack((u, v, w), dim=2)\n",
    "\n",
    "# Example usage\n",
    "points_3d_example = points_3d\n",
    "intrins_example = [\n",
    "    [956.9475, 0.0, 693.9767],\n",
    "    [0.0, 952.2352, 238.6081],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]\n",
    "\n",
    "pixels = _3d_to_pixel(points_3d_example, intrins_example)\n",
    "print(pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_t_example\n",
      " tensor([[[2, 7, 6],\n",
      "         [4, 6, 5]]])\n",
      "I_t_example.shape torch.Size([1, 2, 3])\n",
      "dof_vec_example\n",
      " tensor([[0, 4, 0, 3, 8, 4]])\n",
      "dof_vec_example.shape torch.Size([1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4927, 13.0113, -1.2582],\n",
       "         [-1.9954, 11.8566, -3.3604]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test dimensions\n",
    "torch.manual_seed(42)\n",
    "batch = 1\n",
    "num_points = 2\n",
    "I_t_example = torch.randint(0, 10, (batch, num_points, 3))\n",
    "dof_vec_example = torch.randint(0, 10, (batch, 6))\n",
    "\n",
    "print('I_t_example\\n', I_t_example)\n",
    "print('I_t_example.shape', I_t_example.shape)\n",
    "print(\"dof_vec_example\\n\", dof_vec_example)\n",
    "print('dof_vec_example.shape', dof_vec_example.shape)\n",
    "\n",
    "step_cloud(I_t_example, dof_vec_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Point Cloud:\n",
      " tensor([[[0.4340, 0.1371, 0.5117],\n",
      "         [0.1585, 0.0758, 0.2247],\n",
      "         [0.0624, 0.1816, 0.9998],\n",
      "         [0.5944, 0.6541, 0.0337],\n",
      "         [0.1716, 0.3336, 0.5782]],\n",
      "\n",
      "        [[0.0600, 0.2846, 0.2007],\n",
      "         [0.5014, 0.3139, 0.4654],\n",
      "         [0.1612, 0.1568, 0.2083],\n",
      "         [0.3289, 0.1054, 0.9192],\n",
      "         [0.4008, 0.9302, 0.6558]]])\n",
      "Transformed Point Cloud:\n",
      " tensor([[[1.4340, 0.1371, 0.5117],\n",
      "         [1.1585, 0.0758, 0.2247],\n",
      "         [1.0624, 0.1816, 0.9998],\n",
      "         [1.5944, 0.6541, 0.0337],\n",
      "         [1.1716, 0.3336, 0.5782]],\n",
      "\n",
      "        [[0.0600, 1.2846, 0.2007],\n",
      "         [0.5014, 1.3139, 0.4654],\n",
      "         [0.1612, 1.1568, 0.2083],\n",
      "         [0.3289, 1.1054, 0.9192],\n",
      "         [0.4008, 1.9302, 0.6558]]])\n",
      "Recovered Point Cloud:\n",
      " tensor([[[0.4340, 0.1371, 0.5117],\n",
      "         [0.1585, 0.0758, 0.2247],\n",
      "         [0.0624, 0.1816, 0.9998],\n",
      "         [0.5944, 0.6541, 0.0337],\n",
      "         [0.1716, 0.3336, 0.5782]],\n",
      "\n",
      "        [[0.0600, 0.2846, 0.2007],\n",
      "         [0.5014, 0.3139, 0.4654],\n",
      "         [0.1612, 0.1568, 0.2083],\n",
      "         [0.3289, 0.1054, 0.9192],\n",
      "         [0.4008, 0.9302, 0.6558]]])\n"
     ]
    }
   ],
   "source": [
    "# unit test, invertible?\n",
    "# Example Input\n",
    "I_t = torch.rand(2, 5, 3)  # Original Point Cloud\n",
    "dof_vec = torch.tensor([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]], dtype=torch.float32)  # Translation only\n",
    "\n",
    "# Step 1: Transform the point cloud\n",
    "transformed_cloud = step_cloud(I_t, dof_vec)\n",
    "\n",
    "# Step 2: Compute the inverse 6DoF\n",
    "inverse_dof_vec = inverse_dof(dof_vec)\n",
    "\n",
    "# Step 3: Apply the inverse transformation\n",
    "recovered_cloud = step_cloud(transformed_cloud, inverse_dof_vec)\n",
    "\n",
    "# Check if input matches recovered cloud\n",
    "print(\"Original Point Cloud:\\n\", I_t)\n",
    "print(\"Transformed Point Cloud:\\n\", transformed_cloud)\n",
    "print(\"Recovered Point Cloud:\\n\", recovered_cloud)\n",
    "\n",
    "assert torch.allclose(I_t, recovered_cloud, atol=1e-6), \"The original and recovered point clouds do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 7, 6, 4, 6, 5],\n",
      "        [0, 4, 0, 3, 8, 4]])\n",
      "tensor([[[ 0.2724, -0.5668,  0.7775,  2.0000],\n",
      "         [-0.9207, -0.3882,  0.0395,  7.0000],\n",
      "         [ 0.2794, -0.7267, -0.6276,  6.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.0951, -0.8405,  0.5334,  0.0000],\n",
      "         [ 0.1101,  0.5414,  0.8335,  4.0000],\n",
      "         [-0.9894, -0.0205,  0.1440,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "\n",
    "example_input = torch.randint(0, 10, (batch, 6))\n",
    "\n",
    "print(example_input)\n",
    "print(dof_vec_to_matrix(example_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
